---
title: EPIDP8451 Final Project - An application of machine learning to identify potential
  risk factors for early childhood asthma
author: "Fang Liu (fl2569), Jing Wen (jw4061)"
date: "4/13/2022"
output:
  word_document:
    toc: yes
  html_document:
    hide: yes
    toc: yes
    toc_float: yes
---

------------------------------------------------------------------------

# Reseach Background

> Option 3: group assignment
> Dataset: Exposome

## Research Question

What are the risk factors for predicting asthma diagnosis in children of early school age?

## Research Rationale

Asthma is the most common chronic airway disease of childhood. The etiology of asthma is not fully understood, but family history of the disease, respiratory infections, as well as environmental factors (e.g. airborne pollutants) may contribute to disease development. This study aims to identify modifiable risk factors of asthma for predicting asthma during early school age.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(rpart.plot) # for creating classification tree outputs
library(glmnet)
library(Amelia)
library(pROC) #for calculating ROC/AUC
library(gbm)
library(stats)
library(factoextra)
library(cluster)
```

------------------------------------------------------------------------

# Part 1 - Data preparation

## Data Cleaning & Preprocessing

In this step, we performed data cleaning for final dataset which included: 1) load data, 2) change categorical variables to factors, 3) strip ID variable, 4) check for missing values, 5) check if there is imbalance of outcome distribution, and 6) address collinearity by removing features with >0.8 correlation before modeling.

```{r}
#Load data
load("./exposome.RData")
data <- merge(exposome,phenotype,by="ID") %>% merge(covariates, by="ID")

#Strip off ID Variable
data$ID<-NULL

#make variable as factor
data$hs_asthma <- as.factor(data$hs_asthma) #241 features

# Finding correlated predictors
data_numeric <- data %>% select(where(is.numeric)) #180 features
data_categorical <- data %>% select(!where(is.numeric)) #61 features

correlations <- cor(data_numeric, use="complete.obs")
high.correlations <- findCorrelation(correlations, cutoff=0.8)  

#findCorrelation() searches through a correlation matrix and returns a vector of integers corresponding to COLUMNS to remove to reduce pair-wise correlations

# Remove highly correlated features
data_numeric_low_corr <- data_numeric[,-high.correlations] #159 variables (continuous)

#Combine the low-correlated numerical variables & categorical variables
final_data = bind_cols(data_numeric_low_corr, data_categorical) 
#1301 observations x 220 variables columns

#check missing data
missmap(final_data, main = "Missing values vs observed", x.cex = 0.5, y.cex = 0.5) 

#check if data is balanced
summary(final_data$hs_asthma) #very unbalanced 1159 vs. 142
```

**Conclusion:** In the original dataset, there are 241 features (180 numerical variables + 61 categorical variables); We removed 21 highly correlated numeric variables (180-21=159), so we are left with **220** variables for feature selection for the next step.

## Data Partition

In this step, models would be trained on 70% of patient data (the training set) and tested on the remaining 30% (the test set) as an internal validation of the model.

```{r}
set.seed(100)
train_index <- createDataPartition(y=final_data$hs_asthma, p=0.7, list=FALSE)
train_data <- final_data[train_index,] #912
test_data <- final_data[-train_index,] #389

```

# Part 2 - Variable Selection

## Variable Selection using random forest 

```{r}
set.seed(100)

#hyper parameter tuning: mtry
mtry_val2 <- c(ncol(train_data)-1, sqrt(ncol(train_data)-1), 0.5*ncol(train_data)-1)
mtry_grid <- expand.grid(.mtry=mtry_val2) 

#5-fold cross-validation with upsampling
rf_asthma <- train(hs_asthma ~ ., data=train_data, method="rf", trControl=trainControl("cv", number=5, sampling="up"), metric="Accuracy", tuneGrid=mtry_grid, ntree=100)

rf_asthma$bestTune
rf_asthma$results 

#find which variables are considered "important"
varlist = varImp(rf_asthma)[["importance"]]
important_vars = varlist %>% filter(Overall > 50) %>% arrange(-Overall)

#plot the important variables
plot(varImp(rf_asthma), top=16)
#paste(rownames(important_vars), collapse = " + ")
```

**Conclusion:** Feature selection was done by a first round of 5-fold cross validated Random Forest model on the full predictor set after removing highly-correlated features (220 features). We used the top 16 features from this model's feature importance (i.e., individual features with 50% importance or higher were selected). A complete list of predictor variables is as followed: `r rownames(important_vars)`.

# Part 3 - Construct prediction models

## Model Building & Evaluation

In this step, several machine-learning methods were applied and assessed to predict asthma diagnosis. These methods include traditional logistic regression, Logistic Regression with regularization (Elastic Net), Support Vector Machine (SVM) and ensemble Bagging. To reduce the problem of imbalanced outcome distribution, imbalance corrections were used with Random upsampling which showed the better performance than down-sampling. In addition, to improve the robustness and avoid overfitting (lack of generalization), we used 10-fold cross-validation with group split for the model selection process on the training data.

### Reference Model: Traditional Logistic regression 

Traditional logistic regression was used as reference. Since we were concerned about the over-fitting problem from logistic model, the machine learning models below were presented to avoid over-fitting and prediction.

```{r}
set.seed(100)

logit_asthma <- train(hs_asthma ~ hs_cd_m_Log2 + h_trafnear_preg_pow1over3 + h_builtdens300_preg_Sqrt + hs_pbde153_cadj_Log2 + h_popdens_preg_Sqrt + hs_pfunda_m_Log2 + hs_pfhxs_c_Log2 + hs_ndvi100_s_None + hs_mehp_cadj_Log2 + hs_dmtp_cadj_Log2 + hs_ddt_cadj_Log2 + h_ndvi100_preg_None + hs_dep_madj_Log2 + hs_pm10_dy_hs_h_None + hs_pm25abs_wk_hs_h_Log + hs_cu_c_Log2, data = train_data, method="glm", family = "binomial", trControl=trainControl(method="cv", number = 10))

logit_asthma$results
```

### Model 1: Elastic Net (regularized regression)

Elastic net (EN) was used because it is a hybrid of ridge and lasso regularization. We used regularized regression to avoid the over-fitting problem from traditional regression. It's also useful for identifying important features. The tuning hyperparameter `alpha` and `lambda` in our EN model is **0.8** and **0.00115**, respectively. 

```{r}
set.seed(100)

en_asthma <- train(hs_asthma ~ hs_cd_m_Log2 + h_trafnear_preg_pow1over3 + h_builtdens300_preg_Sqrt + hs_pbde153_cadj_Log2 + h_popdens_preg_Sqrt + hs_pfunda_m_Log2 + hs_pfhxs_c_Log2 + hs_ndvi100_s_None + hs_mehp_cadj_Log2 + hs_dmtp_cadj_Log2 + hs_ddt_cadj_Log2 + h_ndvi100_preg_None + hs_dep_madj_Log2 + hs_pm10_dy_hs_h_None + hs_pm25abs_wk_hs_h_Log + hs_cu_c_Log2,data=train_data, method="glmnet", family="binomial", trControl = trainControl("cv", number = 10, sampling= "up"), tuneLength=10)

en_asthma$bestTune
```

### Model 2: Support Vector Machine

Support Vector Machine (SVM) was used because it is capable of performing classification, regression and outlier detection. `C` is the tuning hyperparameter that controls how much margin can be violated (i.e. how much classifications). Tuning parameter C in our SVM model is **1.333667**.

```{r}
set.seed(100)

#train model 
svm_asthma <- train(hs_asthma ~ hs_cd_m_Log2 + h_trafnear_preg_pow1over3 + h_builtdens300_preg_Sqrt + hs_pbde153_cadj_Log2 + h_popdens_preg_Sqrt + hs_pfunda_m_Log2 + hs_pfhxs_c_Log2 + hs_ndvi100_s_None + hs_mehp_cadj_Log2 + hs_dmtp_cadj_Log2 + hs_ddt_cadj_Log2 + h_ndvi100_preg_None + hs_dep_madj_Log2 + hs_pm10_dy_hs_h_None + hs_pm25abs_wk_hs_h_Log + hs_cu_c_Log2, data = train_data, method="svmLinear", trControl=trainControl(method="cv", number = 10, sampling="up"), preProcess = c("center","scale"), tuneGrid=expand.grid(C=seq(0.001,2,length=10)))

svm_asthma$bestTune
```

### Model 3 - Bagging (ensemble)

Bagging was used because it's the bootstrap aggregation that we can get average results across bootstrapped samples of training data. Compared to the random forest, all features are eligible for selection in bagging, which allows all features to contribute to prediction. Tuning parameter `mtry` in our bagging model is **14.79865**.

```{r}
set.seed(100)

#Note: in bagging, ALL predictor features are eligible for selection at each node 
#parameter tuning
mtry_val1 <- expand.grid(.mtry = c(ncol(train_data)-1, sqrt(ncol(train_data)-1), 0.5*ncol(train_data)-1))

bag_asthma<-train(hs_asthma ~ hs_cd_m_Log2 + h_trafnear_preg_pow1over3 + h_builtdens300_preg_Sqrt + hs_pbde153_cadj_Log2 + h_popdens_preg_Sqrt + hs_pfunda_m_Log2 + hs_pfhxs_c_Log2 + hs_ndvi100_s_None + hs_mehp_cadj_Log2 + hs_dmtp_cadj_Log2 + hs_ddt_cadj_Log2 + h_ndvi100_preg_None + hs_dep_madj_Log2 + hs_pm10_dy_hs_h_None + hs_pm25abs_wk_hs_h_Log + hs_cu_c_Log2, data=train_data, method="rf", metric="Accuracy", trControl = trainControl("cv", number = 10, sampling="up"), tuneGrid=mtry_val1, ntree=100) 

bag_asthma$bestTune
```

## Model comparison

In this step, accuracy was used for model comparison because we already addressed the issue of imbalanced data using upsampling. The model with the highest accuracy value was selected as the final model and the testing data will be used to evaluate the model performance 

```{r}
#compare model performance using accuracy 

confusionMatrix(logit_asthma) #Reference model: 0.8893
confusionMatrix(en_asthma) #0.6338
confusionMatrix(svm_asthma) #0.5833
confusionMatrix(bag_asthma) #0.8761
```

**Conclusion: ** The accuracy measures for each of the models above are as follows: elastic net (**0.6338**), support vector machine (**0.5833**), bagging (**0.8761**); note that the traditional logistic regression model is included as reference. Because the ensemble bagging algorithm has the highest accuracy out of the three machine learning models, the bagging model is chosen as our final model and evaluated using the testing data. 


# Part 4 - Model Evaluation (Bagging)

## Evaluate model performance using testing data

```{r}
asthma_pred = predict(bag_asthma, test_data)
asthma_pred_prob = predict(bag_asthma, test_data, type = "prob") 

#Accuracy
confusionMatrix(asthma_pred, test_data$hs_asthma, positive = "1")

#AUC 
bag_auc = roc(response=test_data$hs_asthma, predictor=asthma_pred_prob[,2])
plot(1-bag_auc$specificities, bag_auc$sensitivities, type="l", ylab="Sensitiviy",xlab="1-Specificity",col="blue",lwd=2, main = "ROC Curve for Asthma")
abline(a=0,b=1)
bag_auc$auc

#Variable importance
plot(varImp(bag_asthma))
```

**Conclusion: ** Recall from pt 3) that ensemble bagging was selected as the final model because it had the highest accuracy out of the three machine learning models that we trained on the training set. Accuracy was **0.8761** for the training set and **0.8689** (95% CI: 0.8312, 0.9008) for the testing set. Area under the curve (AUC) was **`r bag_auc$auc`** with sensitivity and specificity of **0.047619** and **0.9683**, respectively. Model precision, also known as the positive predictive value(PPV) is **0.153846** and model recall is **0.047619**.

According to our variable importance results above, the 5 most important factors for predicting asthma diagnosis are: **building density during pregnancy, Cadmium (Cd) levels in mother during pregnancy, Average of NDVI values within a buffer of 100m at school, population density at pregnancy period, and Traffic density on nearest road during pregnancy.** Other important predictors include Copper (Cu) levels in child, exposure to Polybrominated diphenyl ether-153 (PBDE-153), Diethyl phosphate (DEP) in mother, and other environmental risk factors. Note that variable importance only tells us the relative importance for each variable and not the direction of the effect on our outcome of interest; in other words, we don't know the direction of the association between the variables (e.g., is risk for asthma lower or higher with higher building density during pregnancy?).

# Part 5 - Unsupervised (k-means)

```{r}
set.seed(100)

# Center and scale data
set.up.preprocess<-preProcess(data_numeric_low_corr, method=c("center", "scale"))
transformed.vals<-predict(set.up.preprocess, data_numeric_low_corr)

# Conduct a gap statistic analysis to determine optimal number of clusters 
gap_stat<-clusGap(transformed.vals, FUN=kmeans, nstart=10, K.max=10, B=10, iter.max=50) 
print(gap_stat, method="firstmax")
fviz_gap_stat(gap_stat) 

#perform k-means & visualize the clusters
clusters <- kmeans(transformed.vals, 6, nstart=25)
fviz_cluster(clusters, data=transformed.vals)

#Show the mean value of features within each cluster
clus_centers = clusters$centers

as.data.frame(clus_centers) %>% 
  select(h_builtdens300_preg_Sqrt, hs_cd_m_Log2, hs_ndvi100_s_None, h_popdens_preg_Sqrt, h_trafnear_preg_pow1over3)

#combine original data and the clustering results 
data_with_cluster <- cbind(data_numeric_low_corr,cluster=clusters$cluster)
```

**Conclusion: ** An unsupervised K-means algorithm is used to group the 1,301 mother-child pairs based on their exposure characteristics during and post pregnancy (NOTE: only low-correlated numerical variables are included because k-means utilizes distance measures so categorical variables are excluded). The optimal # of clusters is chosen using the Gap Statistic, which is **6**. In brief, the basic idea of the Gap Statistic is to choose the number of K, where the biggest jump in within-cluster distance occurred. The cluster centers can be used to check if the observations in a cluster is generally above or below average for certain exposures. To illustrate, the observations in cluster #1 are below average in building density, mother cadium exposure, and population density, but is higher in NDVI levels are traffic density; those in cluster #4 are above average in building density and mother cadium levels during pregancy; but have lower levels of exposure to ndvi, lower population and traffic density. There is not really a true way to evaluate the "accuracy" of the clustering results since there is no outcome; nonetheless, findings from these preliminary clustering steps could be used to inform future research questions. 

# Discussion

In this observational cohort study, by using several machine learning algorithms, we identified prediction factors for asthma diagnosis in children, some previously known and others novel. The top factors included building density during pregnancy, Cadmium (Cd) levels in mother during pregnancy, Average of NDVI values within a buffer of 100m at school, population density at pregnancy period, and Traffic density on nearest road during pregnancy. Model evaluation on test data yielded an accuracy of 0.8689 but an AUROC = 0.4646, indicating that the the variables we selected in our model alone may not be sufficient for asthma prediction. Nonetheless, our results indicated that exposure to environmental toxicants, especially in more densely-populated areas, and the nearby environment could be tied to the development of childhood asthma. To improve the health outcomes of adolescents, countries need heightened efforts to tighten the regulation of environmental pollution policies and improve the air quality for future generations. 

The strengths of our study include a large sample size and multiple machine learning methods were fitted and compared with traditional logistic regression approach. The methods were compared based on accuracy and the method with the highest score was selected as final model. Additionally, feature selection was performed before modeling which contributed to a better prediction for the model building part. Limitations of the study include a small testing set and a lack of external data set for validation. This could reduce the generalizability of our model and might only apply to similar healthcare settings in other countries within Europe. As we are unaware of the data collection process, ethical issues may arise if certain population is underrepresented or excluded from the study. 
In addition, even though our final bagging model had a decent accuracy, the model precision and recall is very low, so is the AUC. Thus, this tool needs to be farther developed and evaluated. Additional data on environmental triggers such as weather, pollen count, and air quality might be necessary to improve performance of the short-term predictive model to develop a more useful tool. 
