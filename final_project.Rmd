---
title: "final_project"
author: "Fang Liu"
date: "4/13/2022"
output:
  html_document:
    hide: TRUE
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(rpart.plot) # for creating classification tree outputs
library(glmnet)
library(Amelia)
library(pROC) #for calculating ROC/AUC
library(gbm)
#library(ggbiplot)

library(stats)
library(factoextra)
library(cluster)

```

## Data Cleaning & Preprocessing 
```{r}
#Load data using path of where file is stored
load("exposome.RData")
data <- merge(exposome,phenotype,by="ID") %>% merge(covariates, by="ID")

#Strip off ID Variable
data$ID<-NULL

#make variable as factor
data$hs_asthma <- as.factor(data$hs_asthma)

# Finding correlated predictors
data_numeric <- data %>% select(where(is.numeric))
data_categorical <- data %>% select(!where(is.numeric))

correlations <- cor(data_numeric, use="complete.obs")
high.correlations <- findCorrelation(correlations, cutoff=0.8)  

# findCorrelation() searches through a correlation matrix and returns a vector of integers corresponding to COLUMNS to remove to reduce pair-wise correlations

# Remove highly correlated features
data_numeric_low_corr <- data_numeric[,-high.correlations] #241 variables -> 145 variables (continuous)

#Combine the low-correlated numerical variables & categorical variables
final_data = bind_cols(data_numeric_low_corr, data_categorical) 
#1301 observations x 220 variables columns
```

> 241 variables = 180 numerical variables + 61 categorical variables
> After removing 21 highly correlated numeric variables (180-21=159), we are left with 220 variables.

## Data Partition 
```{r}
set.seed(100)
train_index <- createDataPartition(y=final_data$hs_asthma, p=0.7, list=FALSE)
train_data <- final_data[train_index,] #912
test_data <- final_data[-train_index,] #389

summary(final_data$hs_asthma) #very unbalanced 1159 vs. 142
#str(train_data$hs_asthma)
```


## Variable Selection

### Random Forest
```{r}
set.seed(100)

#hyper parameter tuning: mtry
mtry_val2 <- c(ncol(train_data)-1, sqrt(ncol(train_data)-1), 0.5*ncol(train_data)-1)
mtry_grid <- expand.grid(.mtry=mtry_val2) 

#5-fold cross-validation with upsampling
rf_asthma <- train(hs_asthma ~ ., data=train_data, method="rf", trControl=trainControl("cv", number=5, sampling="up"), metric="Accuracy", tuneGrid=mtry_grid, ntree=100)

rf_asthma$bestTune
rf_asthma$results 
rf_asthma$finalModel
confusionMatrix(rf_asthma)

#find which variables are considered "important"
varlist = varImp(rf_asthma)[["importance"]]
important_vars = varlist %>% filter(Overall > 50) %>% arrange(-Overall)

#predictors(rf_asthma)
plot(varImp(rf_asthma), top=16)
#plot(rf_asthma)

```

```{r}
#OPTION 1 - Only keep the variables deemed to be important by random forest in the model
set.seed(123)
train_data2 = train_data %>% select(rownames(important_vars), hs_asthma)

rf2_asthma <- train(hs_asthma ~ ., data=train_data2, method="rf", trControl=trainControl("cv", number=10, sampling="up"), metric="Accuracy", tuneGrid=mtry_grid, ntree=100)

#rf2_asthma[["resample"]]

rf2_asthma$results
confusionMatrix(rf2_asthma) #0.8695


#OPTION 2 - keep the original data, just change the model formula
set.seed(123)
#paste(rownames(important_vars), collapse = " + ")

rf3_asthma <- train(hs_asthma ~ hs_cd_m_Log2 + h_trafnear_preg_pow1over3 + h_builtdens300_preg_Sqrt + hs_pbde153_cadj_Log2 + h_popdens_preg_Sqrt + hs_pfunda_m_Log2 + hs_pfhxs_c_Log2 + hs_ndvi100_s_None + hs_mehp_cadj_Log2 + hs_dmtp_cadj_Log2 + hs_ddt_cadj_Log2 + h_ndvi100_preg_None + hs_dep_madj_Log2 + hs_pm10_dy_hs_h_None + hs_pm25abs_wk_hs_h_Log + hs_cu_c_Log2, data=train_data, method="rf", trControl=trainControl("cv", number=10, sampling="up"), metric="Accuracy", tuneGrid=mtry_grid, ntree=100)

rf3_asthma$results
confusionMatrix(rf3_asthma)

```

## Model Building 
### Model 1: Elastic Net
```{r}
set.seed(100)
#create 10*10 pairs of parameters for tune length in elastic net model
EN_asthma<- train(
  hs_asthma ~., data = train_data, method = "glmnet",family="binomial",
  trControl = trainControl("cv", number = 10), preProc=c("center", "scale"),
  tuneLength=10  )
```

### Model 2: Bagging/Boosting
```{r}

```

### Model 3: Unsupervised (k-means)
```{r}
set.seed(100)

#exposome_test <- exposome %>% select(-1) %>% select(where(is.numeric))

#scale data
set.up.preprocess<-preProcess(data_numeric_low_corr, method=c("center", "scale"))
transformed.vals<-predict(set.up.preprocess, data_numeric_low_corr)

#Conduct a gap statistic analysis to determine optimal number of clusters 
gap_stat<-clusGap(transformed.vals, FUN=kmeans, nstart=10, K.max=10, B=10, iter.max=50) 
print(gap_stat, method="firstmax")
fviz_gap_stat(gap_stat) #6

#perform k-means & visualize the clusters
clusters<-kmeans(transformed.vals, 6, nstart=25)
str(clusters)
fviz_cluster(clusters, data=transformed.vals)
#Show the mean value of features within each cluster
clusters$centers

#combine original data and the clustering results 
data_with_cluster <- cbind(data_numeric_low_corr,cluster=clusters$cluster)
```


